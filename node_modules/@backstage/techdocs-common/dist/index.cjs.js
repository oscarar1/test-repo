'use strict';

Object.defineProperty(exports, '__esModule', { value: true });

var path = require('path');
var stream = require('stream');
var child_process = require('child_process');
var fs = require('fs-extra');
var yaml = require('js-yaml');
var errors = require('@backstage/errors');
var parseGitUrl = require('git-url-parse');
var backendCommon = require('@backstage/backend-common');
var catalogModel = require('@backstage/catalog-model');
var os = require('os');
var fetch = require('cross-fetch');
var integration = require('@backstage/integration');
var aws = require('aws-sdk');
var JSON5 = require('json5');
var createLimiter = require('p-limit');
var mime = require('mime-types');
var recursiveReadDir = require('recursive-readdir');
var identity = require('@azure/identity');
var storageBlob = require('@azure/storage-blob');
var storage = require('@google-cloud/storage');
var express = require('express');
var pkgcloud = require('pkgcloud');

function _interopDefaultLegacy (e) { return e && typeof e === 'object' && 'default' in e ? e : { 'default': e }; }

var path__default = /*#__PURE__*/_interopDefaultLegacy(path);
var fs__default = /*#__PURE__*/_interopDefaultLegacy(fs);
var yaml__default = /*#__PURE__*/_interopDefaultLegacy(yaml);
var parseGitUrl__default = /*#__PURE__*/_interopDefaultLegacy(parseGitUrl);
var os__default = /*#__PURE__*/_interopDefaultLegacy(os);
var fetch__default = /*#__PURE__*/_interopDefaultLegacy(fetch);
var aws__default = /*#__PURE__*/_interopDefaultLegacy(aws);
var JSON5__default = /*#__PURE__*/_interopDefaultLegacy(JSON5);
var createLimiter__default = /*#__PURE__*/_interopDefaultLegacy(createLimiter);
var mime__default = /*#__PURE__*/_interopDefaultLegacy(mime);
var recursiveReadDir__default = /*#__PURE__*/_interopDefaultLegacy(recursiveReadDir);
var express__default = /*#__PURE__*/_interopDefaultLegacy(express);

function getGeneratorKey(entity) {
  if (!entity) {
    throw new Error("No entity provided");
  }
  return "techdocs";
}
const runCommand = async ({
  command,
  args,
  options,
  logStream = new stream.PassThrough()
}) => {
  await new Promise((resolve, reject) => {
    const process = child_process.spawn(command, args, options);
    process.stdout.on("data", (stream) => {
      logStream.write(stream);
    });
    process.stderr.on("data", (stream) => {
      logStream.write(stream);
    });
    process.on("error", (error) => {
      return reject(error);
    });
    process.on("close", (code) => {
      if (code !== 0) {
        return reject(`Command ${command} failed, exit code: ${code}`);
      }
      return resolve();
    });
  });
};
const isValidRepoUrlForMkdocs = (repoUrl, locationType) => {
  const cleanRepoUrl = repoUrl.replace(/\/$/, "");
  if (locationType === "github" || locationType === "gitlab") {
    return cleanRepoUrl.split("/").length === 5;
  }
  return false;
};
const getRepoUrlFromLocationAnnotation = (parsedLocationAnnotation) => {
  const {type: locationType, target} = parsedLocationAnnotation;
  const supportedHosts = ["github", "gitlab"];
  if (supportedHosts.includes(locationType)) {
    return target.replace(/.git\/*$/, "");
  }
  return void 0;
};
class UnknownTag {
  constructor(data, type) {
    this.data = data;
    this.type = type;
  }
}
const MKDOCS_SCHEMA = yaml.DEFAULT_SCHEMA.extend([
  new yaml.Type("", {
    kind: "scalar",
    multi: true,
    representName: (o) => o.type,
    represent: (o) => {
      var _a;
      return (_a = o.data) != null ? _a : "";
    },
    instanceOf: UnknownTag,
    construct: (data, type) => new UnknownTag(data, type)
  })
]);
const validateMkdocsYaml = async (mkdocsYmlPath) => {
  let mkdocsYmlFileString;
  try {
    mkdocsYmlFileString = await fs__default['default'].readFile(mkdocsYmlPath, "utf8");
  } catch (error) {
    throw new Error(`Could not read MkDocs YAML config file ${mkdocsYmlPath} before for validation: ${error.message}`);
  }
  const mkdocsYml = yaml__default['default'].load(mkdocsYmlFileString, {
    schema: MKDOCS_SCHEMA
  });
  if (mkdocsYml.docs_dir && path.isAbsolute(path.normalize(mkdocsYml.docs_dir))) {
    throw new Error("docs_dir configuration value in mkdocs can't be an absolute directory path for security reasons. Use relative paths instead which are resolved relative to your mkdocs.yml file location.");
  }
};
const patchMkdocsYmlPreBuild = async (mkdocsYmlPath, logger, parsedLocationAnnotation) => {
  let mkdocsYmlFileString;
  try {
    mkdocsYmlFileString = await fs__default['default'].readFile(mkdocsYmlPath, "utf8");
  } catch (error) {
    logger.warn(`Could not read MkDocs YAML config file ${mkdocsYmlPath} before running the generator: ${error.message}`);
    return;
  }
  let mkdocsYml;
  try {
    mkdocsYml = yaml__default['default'].load(mkdocsYmlFileString, {schema: MKDOCS_SCHEMA});
    if (typeof mkdocsYml === "string" || typeof mkdocsYml === "undefined") {
      throw new Error("Bad YAML format.");
    }
  } catch (error) {
    logger.warn(`Error in parsing YAML at ${mkdocsYmlPath} before running the generator. ${error.message}`);
    return;
  }
  if (!("repo_url" in mkdocsYml)) {
    const repoUrl = getRepoUrlFromLocationAnnotation(parsedLocationAnnotation);
    if (repoUrl !== void 0) {
      if (isValidRepoUrlForMkdocs(repoUrl, parsedLocationAnnotation.type)) {
        mkdocsYml.repo_url = repoUrl;
      }
    }
  }
  try {
    await fs__default['default'].writeFile(mkdocsYmlPath, yaml__default['default'].dump(mkdocsYml, {schema: MKDOCS_SCHEMA}), "utf8");
  } catch (error) {
    logger.warn(`Could not write to ${mkdocsYmlPath} after updating it before running the generator. ${error.message}`);
    return;
  }
};
const addBuildTimestampMetadata = async (techdocsMetadataPath, logger) => {
  try {
    await fs__default['default'].access(techdocsMetadataPath, fs__default['default'].constants.F_OK);
  } catch (err) {
    await fs__default['default'].writeJson(techdocsMetadataPath, JSON.parse("{}"));
  }
  let json;
  try {
    json = await fs__default['default'].readJson(techdocsMetadataPath);
  } catch (err) {
    const message = `Invalid JSON at ${techdocsMetadataPath} with error ${err.message}`;
    logger.error(message);
    throw new Error(message);
  }
  json.build_timestamp = Date.now();
  await fs__default['default'].writeJson(techdocsMetadataPath, json);
  return;
};
const storeEtagMetadata = async (techdocsMetadataPath, etag) => {
  const json = await fs__default['default'].readJson(techdocsMetadataPath);
  json.etag = etag;
  await fs__default['default'].writeJson(techdocsMetadataPath, json);
};

const createStream = () => {
  const log = [];
  const stream$1 = new stream.PassThrough();
  stream$1.on("data", (chunk) => {
    const textValue = chunk.toString().trim();
    if ((textValue == null ? void 0 : textValue.length) > 1)
      log.push(textValue);
  });
  return [log, stream$1];
};
class TechdocsGenerator {
  constructor({
    logger,
    containerRunner,
    config
  }) {
    var _a;
    this.logger = logger;
    this.options = {
      runGeneratorIn: (_a = config.getOptionalString("techdocs.generators.techdocs")) != null ? _a : "docker"
    };
    this.containerRunner = containerRunner;
  }
  async run({
    inputDir,
    outputDir,
    parsedLocationAnnotation,
    etag
  }) {
    const [log, logStream] = createStream();
    const mkdocsYmlPath = path__default['default'].join(inputDir, "mkdocs.yml");
    if (parsedLocationAnnotation) {
      await patchMkdocsYmlPreBuild(mkdocsYmlPath, this.logger, parsedLocationAnnotation);
    }
    await validateMkdocsYaml(mkdocsYmlPath);
    const mountDirs = {
      [inputDir]: "/input",
      [outputDir]: "/output"
    };
    try {
      switch (this.options.runGeneratorIn) {
        case "local":
          await runCommand({
            command: "mkdocs",
            args: ["build", "-d", outputDir, "-v"],
            options: {
              cwd: inputDir
            },
            logStream
          });
          this.logger.info(`Successfully generated docs from ${inputDir} into ${outputDir} using local mkdocs`);
          break;
        case "docker":
          await this.containerRunner.runContainer({
            imageName: "spotify/techdocs",
            args: ["build", "-d", "/output"],
            logStream,
            mountDirs,
            workingDir: "/input",
            envVars: {HOME: "/tmp"}
          });
          this.logger.info(`Successfully generated docs from ${inputDir} into ${outputDir} using techdocs-container`);
          break;
        default:
          throw new Error(`Invalid config value "${this.options.runGeneratorIn}" provided in 'techdocs.generators.techdocs'.`);
      }
    } catch (error) {
      this.logger.debug(`Failed to generate docs from ${inputDir} into ${outputDir}`);
      this.logger.error(`Build failed with error: ${log}`);
      throw new Error(`Failed to generate docs from ${inputDir} into ${outputDir} with error ${error.message}`);
    }
    await addBuildTimestampMetadata(path__default['default'].join(outputDir, "techdocs_metadata.json"), this.logger);
    if (etag) {
      await storeEtagMetadata(path__default['default'].join(outputDir, "techdocs_metadata.json"), etag);
    }
  }
}

class Generators {
  constructor() {
    this.generatorMap = new Map();
  }
  static async fromConfig(config, {
    logger,
    containerRunner
  }) {
    const generators = new Generators();
    const techdocsGenerator = new TechdocsGenerator({
      logger,
      containerRunner,
      config
    });
    generators.register("techdocs", techdocsGenerator);
    return generators;
  }
  register(generatorKey, generator) {
    this.generatorMap.set(generatorKey, generator);
  }
  get(entity) {
    const generatorKey = getGeneratorKey(entity);
    const generator = this.generatorMap.get(generatorKey);
    if (!generator) {
      throw new Error(`No generator registered for entity: "${generatorKey}"`);
    }
    return generator;
  }
}

function getGitHost(url) {
  const {resource} = parseGitUrl__default['default'](url);
  return resource;
}
function getGitRepoType(url) {
  var _a;
  const typeMapping = [
    {url: /github*/g, type: "github"},
    {url: /gitlab*/g, type: "gitlab"},
    {url: /azure*/g, type: "azure/api"}
  ];
  const type = (_a = typeMapping.filter((item) => item.url.test(url))[0]) == null ? void 0 : _a.type;
  return type;
}
const getGitHubIntegrationConfig = (config, host) => {
  var _a;
  const allGitHubConfigs = integration.readGitHubIntegrationConfigs((_a = config.getOptionalConfigArray("integrations.github")) != null ? _a : []);
  const gitHubIntegrationConfig = allGitHubConfigs.find((v) => v.host === host);
  if (!gitHubIntegrationConfig) {
    throw new Error(`Unable to locate GitHub integration for the host ${host}`);
  }
  return gitHubIntegrationConfig;
};
const getGitLabIntegrationConfig = (config, host) => {
  var _a;
  const allGitLabConfigs = integration.readGitLabIntegrationConfigs((_a = config.getOptionalConfigArray("integrations.gitlab")) != null ? _a : []);
  const gitLabIntegrationConfig = allGitLabConfigs.find((v) => v.host === host);
  if (!gitLabIntegrationConfig) {
    throw new Error(`Unable to locate GitLab integration for the host ${host}`);
  }
  return gitLabIntegrationConfig;
};
const getAzureIntegrationConfig = (config, host) => {
  var _a;
  const allAzureIntegrationConfig = integration.readAzureIntegrationConfigs((_a = config.getOptionalConfigArray("integrations.azure")) != null ? _a : []);
  const azureIntegrationConfig = allAzureIntegrationConfig.find((v) => v.host === host);
  if (!azureIntegrationConfig) {
    throw new Error(`Unable to locate Azure integration for the host ${host}`);
  }
  return azureIntegrationConfig;
};
const getTokenForGitRepo = async (repositoryUrl, config) => {
  const host = getGitHost(repositoryUrl);
  const type = getGitRepoType(repositoryUrl);
  try {
    switch (type) {
      case "github":
        return getGitHubIntegrationConfig(config, host).token;
      case "gitlab":
        return getGitLabIntegrationConfig(config, host).token;
      case "azure/api":
        return getAzureIntegrationConfig(config, host).token;
      default:
        throw new Error("Failed to get repository type");
    }
  } catch (error) {
    throw error;
  }
};

function getGithubApiUrl(config, url) {
  var _a, _b, _c;
  const {resource, owner, name} = parseGitUrl__default['default'](url);
  const providerConfigs = (_a = config.getOptionalConfigArray("integrations.github")) != null ? _a : [];
  const hostConfig = providerConfigs.filter((providerConfig) => providerConfig.getOptionalString("host") === resource);
  const apiBaseUrl = (_c = (_b = hostConfig[0]) == null ? void 0 : _b.getOptionalString("apiBaseUrl")) != null ? _c : "https://api.github.com";
  const apiRepos = "repos";
  return new URL(`${apiBaseUrl}/${apiRepos}/${owner}/${name}`);
}
function getGitlabApiUrl(url) {
  const {protocol, resource, full_name: fullName} = parseGitUrl__default['default'](url);
  const apiProjectsBasePath = "api/v4/projects";
  const project = encodeURIComponent(fullName);
  const branches = "repository/branches";
  return new URL(`${protocol}://${resource}/${apiProjectsBasePath}/${project}/${branches}`);
}
function getAzureApiUrl(url) {
  const {protocol, resource, organization, owner, name} = parseGitUrl__default['default'](url);
  const apiRepoPath = "_apis/git/repositories";
  const apiVersion = "api-version=6.0";
  return new URL(`${protocol}://${resource}/${organization}/${owner}/${apiRepoPath}/${name}?${apiVersion}`);
}
async function getGithubDefaultBranch(repositoryUrl, config) {
  const path = getGithubApiUrl(config, repositoryUrl).toString();
  const host = getGitHost(repositoryUrl);
  const integrationConfig = getGitHubIntegrationConfig(config, host);
  const options = integration.getGitHubRequestOptions(integrationConfig);
  try {
    const raw = await fetch__default['default'](path, options);
    if (!raw.ok) {
      throw new Error(`Failed to load url: ${raw.status} ${raw.statusText}. Make sure you have permission to repository: ${repositoryUrl}`);
    }
    const {default_branch: branch} = await raw.json();
    if (!branch) {
      throw new Error("Not found github default branch");
    }
    return branch;
  } catch (error) {
    throw new Error(`Failed to get github default branch: ${error}`);
  }
}
async function getGitlabDefaultBranch(repositoryUrl, config) {
  const path = getGitlabApiUrl(repositoryUrl).toString();
  const host = getGitHost(repositoryUrl);
  const integrationConfig = getGitLabIntegrationConfig(config, host);
  const options = integration.getGitLabRequestOptions(integrationConfig);
  try {
    const raw = await fetch__default['default'](path, options);
    if (!raw.ok) {
      throw new Error(`Failed to load url: ${raw.status} ${raw.statusText}. Make sure you have permission to repository: ${repositoryUrl}`);
    }
    const result = await raw.json();
    const {name} = (result || []).find((branch) => branch.default === true);
    if (!name) {
      throw new Error("Not found gitlab default branch");
    }
    return name;
  } catch (error) {
    throw new Error(`Failed to get gitlab default branch: ${error}`);
  }
}
async function getAzureDefaultBranch(repositoryUrl, config) {
  const path = getAzureApiUrl(repositoryUrl).toString();
  const host = getGitHost(repositoryUrl);
  const integrationConfig = getAzureIntegrationConfig(config, host);
  const options = integration.getAzureRequestOptions(integrationConfig);
  try {
    const urlResponse = await fetch__default['default'](path, options);
    if (!urlResponse.ok) {
      throw new Error(`Failed to load url: ${urlResponse.status} ${urlResponse.statusText}. Make sure you have permission to repository: ${repositoryUrl}`);
    }
    const urlResult = await urlResponse.json();
    const idResponse = await fetch__default['default'](urlResult.url, options);
    if (!idResponse.ok) {
      throw new Error(`Failed to load url: ${idResponse.status} ${idResponse.statusText}. Make sure you have permission to repository: ${urlResult.repository.url}`);
    }
    const idResult = await idResponse.json();
    const name = idResult.defaultBranch;
    if (!name) {
      throw new Error("Not found Azure DevOps default branch");
    }
    return name;
  } catch (error) {
    throw new Error(`Failed to get Azure DevOps default branch: ${error}`);
  }
}
const getDefaultBranch = async (repositoryUrl, config) => {
  const type = getGitRepoType(repositoryUrl);
  try {
    switch (type) {
      case "github":
        return await getGithubDefaultBranch(repositoryUrl, config);
      case "gitlab":
        return await getGitlabDefaultBranch(repositoryUrl, config);
      case "azure/api":
        return await getAzureDefaultBranch(repositoryUrl, config);
      default:
        throw new Error("Failed to get repository type");
    }
  } catch (error) {
    throw error;
  }
};

const parseReferenceAnnotation = (annotationName, entity) => {
  var _a;
  const annotation = (_a = entity.metadata.annotations) == null ? void 0 : _a[annotationName];
  if (!annotation) {
    throw new errors.InputError(`No location annotation provided in entity: ${entity.metadata.name}`);
  }
  const {type, target} = catalogModel.parseLocationReference(annotation);
  return {
    type,
    target
  };
};
const getLocationForEntity = (entity) => {
  const {type, target} = parseReferenceAnnotation("backstage.io/techdocs-ref", entity);
  switch (type) {
    case "github":
    case "gitlab":
    case "azure/api":
    case "url":
      return {type, target};
    case "dir":
      if (path__default['default'].isAbsolute(target)) {
        return {type, target};
      }
      return parseReferenceAnnotation("backstage.io/managed-by-location", entity);
    default:
      throw new Error(`Invalid reference annotation ${type}`);
  }
};
const getGitRepositoryTempFolder = async (repositoryUrl, config) => {
  const parsedGitLocation = parseGitUrl__default['default'](repositoryUrl);
  parsedGitLocation.git_suffix = false;
  if (!parsedGitLocation.ref) {
    parsedGitLocation.ref = await getDefaultBranch(parsedGitLocation.toString("https"), config);
  }
  return path__default['default'].join(fs__default['default'].realpathSync(os__default['default'].tmpdir()), "backstage-repo", parsedGitLocation.resource, parsedGitLocation.owner, parsedGitLocation.name, parsedGitLocation.ref);
};
const checkoutGitRepository = async (repoUrl, config, logger) => {
  const parsedGitLocation = parseGitUrl__default['default'](repoUrl);
  const repositoryTmpPath = await getGitRepositoryTempFolder(repoUrl, config);
  const token = await getTokenForGitRepo(repoUrl, config);
  let git = backendCommon.Git.fromAuth({logger});
  if (token) {
    const type = getGitRepoType(repoUrl);
    switch (type) {
      case "github":
        git = backendCommon.Git.fromAuth({
          username: "x-access-token",
          password: token,
          logger
        });
        parsedGitLocation.token = `x-access-token:${token}`;
        break;
      case "gitlab":
        git = backendCommon.Git.fromAuth({
          username: "oauth2",
          password: token,
          logger
        });
        parsedGitLocation.token = `dummyUsername:${token}`;
        parsedGitLocation.git_suffix = true;
        break;
      case "azure/api":
        git = backendCommon.Git.fromAuth({
          username: "notempty",
          password: token,
          logger
        });
        break;
      default:
        parsedGitLocation.token = `:${token}`;
    }
  }
  if (fs__default['default'].existsSync(repositoryTmpPath)) {
    try {
      const currentBranchName = await git.currentBranch({
        dir: repositoryTmpPath
      });
      await git.fetch({dir: repositoryTmpPath, remote: "origin"});
      await git.merge({
        dir: repositoryTmpPath,
        theirs: `origin/${currentBranchName}`,
        ours: currentBranchName || void 0,
        author: {name: "Backstage TechDocs", email: "techdocs@backstage.io"},
        committer: {
          name: "Backstage TechDocs",
          email: "techdocs@backstage.io"
        }
      });
      return repositoryTmpPath;
    } catch (e) {
      logger.info(`Found error "${e.message}" in cached repository "${repoUrl}" when getting latest changes. Removing cached repository.`);
      fs__default['default'].removeSync(repositoryTmpPath);
    }
  }
  const repositoryCheckoutUrl = parsedGitLocation.toString("https");
  fs__default['default'].mkdirSync(repositoryTmpPath, {recursive: true});
  await git.clone({url: repositoryCheckoutUrl, dir: repositoryTmpPath});
  return repositoryTmpPath;
};
const getLastCommitTimestamp = async (repositoryLocation, logger) => {
  const git = backendCommon.Git.fromAuth({logger});
  const sha = await git.resolveRef({dir: repositoryLocation, ref: "HEAD"});
  const commit = await git.readCommit({dir: repositoryLocation, sha});
  return commit.commit.committer.timestamp;
};
const getDocFilesFromRepository = async (reader, entity, opts) => {
  var _a, _b;
  const {target} = parseReferenceAnnotation("backstage.io/techdocs-ref", entity);
  (_a = opts == null ? void 0 : opts.logger) == null ? void 0 : _a.debug(`Reading files from ${target}`);
  const readTreeResponse = await reader.readTree(target, {etag: opts == null ? void 0 : opts.etag});
  const preparedDir = await readTreeResponse.dir();
  (_b = opts == null ? void 0 : opts.logger) == null ? void 0 : _b.debug(`Tree downloaded and stored at ${preparedDir}`);
  return {
    preparedDir,
    etag: readTreeResponse.etag
  };
};

class DirectoryPreparer {
  constructor(config, logger, reader) {
    this.config = config;
    this.logger = logger;
    this.reader = reader;
    this.config = config;
    this.logger = logger;
    this.reader = reader;
  }
  async resolveManagedByLocationToDir(entity, options) {
    const {type, target} = parseReferenceAnnotation("backstage.io/managed-by-location", entity);
    this.logger.debug(`Building docs for entity with type 'dir' and managed-by-location '${type}'`);
    switch (type) {
      case "url": {
        const response = await this.reader.readTree(target, {
          etag: options == null ? void 0 : options.etag
        });
        const preparedDir = await response.dir();
        return {
          preparedDir,
          etag: response.etag
        };
      }
      case "github":
      case "gitlab":
      case "azure/api": {
        const parsedGitLocation = parseGitUrl__default['default'](target);
        const repoLocation = await checkoutGitRepository(target, this.config, this.logger);
        const etag = await getLastCommitTimestamp(repoLocation, this.logger);
        if ((options == null ? void 0 : options.etag) === etag.toString()) {
          throw new errors.NotModifiedError();
        }
        return {
          preparedDir: path__default['default'].dirname(path__default['default'].join(repoLocation, parsedGitLocation.filepath)),
          etag: etag.toString()
        };
      }
      case "file":
        return {
          preparedDir: path__default['default'].dirname(target),
          etag: ""
        };
      default:
        throw new errors.InputError(`Unable to resolve location type ${type}`);
    }
  }
  async prepare(entity) {
    this.logger.warn("You are using the legacy dir preparer in TechDocs which will be removed in near future (March 2021). Migrate to URL reader by updating `backstage.io/techdocs-ref` annotation in `catalog-info.yaml` to be prefixed with `url:`. Read the migration guide and benefits at https://github.com/backstage/backstage/issues/4409 ");
    const {target} = parseReferenceAnnotation("backstage.io/techdocs-ref", entity);
    const response = await this.resolveManagedByLocationToDir(entity);
    return {
      preparedDir: path__default['default'].resolve(response.preparedDir, target),
      etag: response.etag
    };
  }
}

class CommonGitPreparer {
  constructor(config, logger) {
    this.config = config;
    this.logger = logger;
  }
  async prepare(entity, options) {
    this.logger.warn(`You are using the legacy git preparer in TechDocs for \`${entity.metadata.name}\` which will be removed in near future (March 2021). Migrate to URL reader by updating \`backstage.io/techdocs-ref\` annotation in \`catalog-info.yaml\` to be prefixed with \`url:\`. Read the migration guide and benefits at https://github.com/backstage/backstage/issues/4409 `);
    const {target} = parseReferenceAnnotation("backstage.io/techdocs-ref", entity);
    try {
      const repoPath = await checkoutGitRepository(target, this.config, this.logger);
      const etag = await getLastCommitTimestamp(repoPath, this.logger);
      if ((options == null ? void 0 : options.etag) === etag.toString()) {
        throw new errors.NotModifiedError();
      }
      const parsedGitLocation = parseGitUrl__default['default'](target);
      return {
        preparedDir: path__default['default'].join(repoPath, parsedGitLocation.filepath),
        etag: etag.toString()
      };
    } catch (error) {
      if (error instanceof errors.NotModifiedError) {
        this.logger.debug(`Cache is valid for etag ${options == null ? void 0 : options.etag}`);
      } else {
        this.logger.debug(`Repo checkout failed with error ${error.message}`);
      }
      throw error;
    }
  }
}

class UrlPreparer {
  constructor(reader, logger) {
    this.logger = logger;
    this.reader = reader;
  }
  async prepare(entity, options) {
    try {
      return await getDocFilesFromRepository(this.reader, entity, {
        etag: options == null ? void 0 : options.etag,
        logger: this.logger
      });
    } catch (error) {
      if (error instanceof errors.NotModifiedError) {
        this.logger.debug(`Cache is valid for etag ${options == null ? void 0 : options.etag}`);
      } else {
        this.logger.debug(`Unable to fetch files for building docs ${error.message}`);
      }
      throw error;
    }
  }
}

class Preparers {
  constructor() {
    this.preparerMap = new Map();
  }
  static async fromConfig(config, {logger, reader}) {
    const preparers = new Preparers();
    const urlPreparer = new UrlPreparer(reader, logger);
    preparers.register("url", urlPreparer);
    const directoryPreparer = new DirectoryPreparer(config, logger, reader);
    preparers.register("dir", directoryPreparer);
    const commonGitPreparer = new CommonGitPreparer(config, logger);
    preparers.register("github", commonGitPreparer);
    preparers.register("gitlab", commonGitPreparer);
    preparers.register("azure/api", commonGitPreparer);
    return preparers;
  }
  register(protocol, preparer) {
    this.preparerMap.set(protocol, preparer);
  }
  get(entity) {
    const {type} = parseReferenceAnnotation("backstage.io/techdocs-ref", entity);
    const preparer = this.preparerMap.get(type);
    if (!preparer) {
      throw new Error(`No preparer registered for type: "${type}"`);
    }
    return preparer;
  }
}

const getContentTypeForExtension = (ext) => {
  const defaultContentType = "text/plain; charset=utf-8";
  if (ext.match(/htm|xml|svg/i)) {
    return defaultContentType;
  }
  return mime__default['default'].contentType(ext) || defaultContentType;
};
const getHeadersForFileExtension = (fileExtension) => {
  return {
    "Content-Type": getContentTypeForExtension(fileExtension)
  };
};
const getFileTreeRecursively = async (rootDirPath) => {
  const fileList = await recursiveReadDir__default['default'](rootDirPath).catch((error) => {
    throw new Error(`Failed to read template directory: ${error.message}`);
  });
  return fileList;
};

const streamToBuffer = (stream) => {
  return new Promise((resolve, reject) => {
    try {
      const chunks = [];
      stream.on("data", (chunk) => chunks.push(chunk));
      stream.on("error", reject);
      stream.on("end", () => resolve(Buffer.concat(chunks)));
    } catch (e) {
      throw new Error(`Unable to parse the response data ${e.message}`);
    }
  });
};
class AwsS3Publish {
  constructor(storageClient, bucketName, logger) {
    this.storageClient = storageClient;
    this.bucketName = bucketName;
    this.logger = logger;
    this.storageClient = storageClient;
    this.bucketName = bucketName;
    this.logger = logger;
  }
  static fromConfig(config, logger) {
    let bucketName = "";
    try {
      bucketName = config.getString("techdocs.publisher.awsS3.bucketName");
    } catch (error) {
      throw new Error("Since techdocs.publisher.type is set to 'awsS3' in your app config, techdocs.publisher.awsS3.bucketName is required.");
    }
    const credentialsConfig = config.getOptionalConfig("techdocs.publisher.awsS3.credentials");
    const credentials = AwsS3Publish.buildCredentials(credentialsConfig);
    const region = config.getOptionalString("techdocs.publisher.awsS3.region");
    const endpoint = config.getOptionalString("techdocs.publisher.awsS3.endpoint");
    const s3ForcePathStyle = config.getOptionalBoolean("techdocs.publisher.awsS3.s3ForcePathStyle");
    const storageClient = new aws__default['default'].S3({
      credentials,
      ...region && {region},
      ...endpoint && {endpoint},
      ...s3ForcePathStyle && {s3ForcePathStyle}
    });
    return new AwsS3Publish(storageClient, bucketName, logger);
  }
  static buildCredentials(config) {
    if (!config) {
      return void 0;
    }
    const accessKeyId = config.getOptionalString("accessKeyId");
    const secretAccessKey = config.getOptionalString("secretAccessKey");
    let explicitCredentials;
    if (accessKeyId && secretAccessKey) {
      explicitCredentials = new aws.Credentials({
        accessKeyId,
        secretAccessKey
      });
    }
    const roleArn = config.getOptionalString("roleArn");
    if (roleArn) {
      return new aws__default['default'].ChainableTemporaryCredentials({
        masterCredentials: explicitCredentials,
        params: {
          RoleSessionName: "backstage-aws-techdocs-s3-publisher",
          RoleArn: roleArn
        }
      });
    }
    return explicitCredentials;
  }
  async getReadiness() {
    try {
      await this.storageClient.headBucket({Bucket: this.bucketName}).promise();
      this.logger.info(`Successfully connected to the AWS S3 bucket ${this.bucketName}.`);
      return {isAvailable: true};
    } catch (error) {
      this.logger.error(`Could not retrieve metadata about the AWS S3 bucket ${this.bucketName}. Make sure the bucket exists. Also make sure that authentication is setup either by explicitly defining credentials and region in techdocs.publisher.awsS3 in app config or by using environment variables. Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage`);
      this.logger.error(`from AWS client library`, error);
      return {
        isAvailable: false
      };
    }
  }
  async publish({entity, directory}) {
    try {
      const allFilesToUpload = await getFileTreeRecursively(directory);
      const limiter = createLimiter__default['default'](10);
      const uploadPromises = [];
      for (const filePath of allFilesToUpload) {
        const relativeFilePath = path__default['default'].relative(directory, filePath);
        const relativeFilePathPosix = relativeFilePath.split(path__default['default'].sep).join(path__default['default'].posix.sep);
        const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
        const destination = `${entityRootDir}/${relativeFilePathPosix}`;
        const uploadFile = limiter(() => {
          const fileStream = fs__default['default'].createReadStream(filePath);
          const params = {
            Bucket: this.bucketName,
            Key: destination,
            Body: fileStream
          };
          return this.storageClient.upload(params).promise();
        });
        uploadPromises.push(uploadFile);
      }
      await Promise.all(uploadPromises);
      this.logger.info(`Successfully uploaded all the generated files for Entity ${entity.metadata.name}. Total number of files: ${allFilesToUpload.length}`);
      return;
    } catch (e) {
      const errorMessage = `Unable to upload file(s) to AWS S3. ${e}`;
      this.logger.error(errorMessage);
      throw new Error(errorMessage);
    }
  }
  async fetchTechDocsMetadata(entityName) {
    try {
      return await new Promise(async (resolve, reject) => {
        const entityRootDir = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;
        const stream = this.storageClient.getObject({
          Bucket: this.bucketName,
          Key: `${entityRootDir}/techdocs_metadata.json`
        }).createReadStream();
        try {
          const techdocsMetadataJson = await streamToBuffer(stream);
          if (!techdocsMetadataJson) {
            throw new Error(`Unable to parse the techdocs metadata file ${entityRootDir}/techdocs_metadata.json.`);
          }
          const techdocsMetadata = JSON5__default['default'].parse(techdocsMetadataJson.toString("utf-8"));
          resolve(techdocsMetadata);
        } catch (err) {
          this.logger.error(err.message);
          reject(new Error(err.message));
        }
      });
    } catch (e) {
      throw new Error(`TechDocs metadata fetch failed, ${e.message}`);
    }
  }
  docsRouter() {
    return async (req, res) => {
      const filePath = decodeURI(req.path.replace(/^\//, ""));
      const fileExtension = path__default['default'].extname(filePath);
      const responseHeaders = getHeadersForFileExtension(fileExtension);
      const stream = this.storageClient.getObject({Bucket: this.bucketName, Key: filePath}).createReadStream();
      try {
        for (const [headerKey, headerValue] of Object.entries(responseHeaders)) {
          res.setHeader(headerKey, headerValue);
        }
        res.send(await streamToBuffer(stream));
      } catch (err) {
        this.logger.warn(err.message);
        res.status(404).json(err.message);
      }
    };
  }
  async hasDocsBeenGenerated(entity) {
    try {
      const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
      await this.storageClient.headObject({
        Bucket: this.bucketName,
        Key: `${entityRootDir}/index.html`
      }).promise();
      return Promise.resolve(true);
    } catch (e) {
      return Promise.resolve(false);
    }
  }
}

const BATCH_CONCURRENCY = 3;
class AzureBlobStoragePublish {
  constructor(storageClient, containerName, logger) {
    this.storageClient = storageClient;
    this.containerName = containerName;
    this.logger = logger;
    this.storageClient = storageClient;
    this.containerName = containerName;
    this.logger = logger;
  }
  static fromConfig(config, logger) {
    let containerName = "";
    try {
      containerName = config.getString("techdocs.publisher.azureBlobStorage.containerName");
    } catch (error) {
      throw new Error("Since techdocs.publisher.type is set to 'azureBlobStorage' in your app config, techdocs.publisher.azureBlobStorage.containerName is required.");
    }
    let accountName = "";
    try {
      accountName = config.getString("techdocs.publisher.azureBlobStorage.credentials.accountName");
    } catch (error) {
      throw new Error("Since techdocs.publisher.type is set to 'azureBlobStorage' in your app config, techdocs.publisher.azureBlobStorage.credentials.accountName is required.");
    }
    const accountKey = config.getOptionalString("techdocs.publisher.azureBlobStorage.credentials.accountKey");
    let credential;
    if (accountKey) {
      credential = new storageBlob.StorageSharedKeyCredential(accountName, accountKey);
    } else {
      credential = new identity.DefaultAzureCredential();
    }
    const storageClient = new storageBlob.BlobServiceClient(`https://${accountName}.blob.core.windows.net`, credential);
    return new AzureBlobStoragePublish(storageClient, containerName, logger);
  }
  async getReadiness() {
    try {
      const response = await this.storageClient.getContainerClient(this.containerName).getProperties();
      if (response._response.status === 200) {
        return {
          isAvailable: true
        };
      }
      if (response._response.status >= 400) {
        this.logger.error(`Failed to retrieve metadata from ${response._response.request.url} with status code ${response._response.status}.`);
      }
    } catch (e) {
      this.logger.error(`from Azure Blob Storage client library: ${e.message}`);
    }
    this.logger.error(`Could not retrieve metadata about the Azure Blob Storage container ${this.containerName}. Make sure that the Azure project and container exist and the access key is setup correctly techdocs.publisher.azureBlobStorage.credentials defined in app config has correct permissions. Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage`);
    return {isAvailable: false};
  }
  async publish({entity, directory}) {
    try {
      const allFilesToUpload = await getFileTreeRecursively(directory);
      const limiter = createLimiter__default['default'](BATCH_CONCURRENCY);
      const promises = allFilesToUpload.map((sourceFilePath) => {
        const relativeFilePath = path__default['default'].normalize(path__default['default'].relative(directory, sourceFilePath));
        const relativeFilePathPosix = relativeFilePath.split(path__default['default'].sep).join(path__default['default'].posix.sep);
        const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
        const destination = `${entityRootDir}/${relativeFilePathPosix}`;
        return limiter(async () => {
          const response = await this.storageClient.getContainerClient(this.containerName).getBlockBlobClient(destination).uploadFile(sourceFilePath);
          if (response._response.status >= 400) {
            return {
              ...response,
              error: new Error(`Upload failed for ${sourceFilePath} with status code ${response._response.status}`)
            };
          }
          return {
            ...response,
            error: void 0
          };
        });
      });
      const responses = await Promise.all(promises);
      const failed = responses.filter((r) => r.error);
      if (failed.length === 0) {
        this.logger.info(`Successfully uploaded the ${responses.length} generated file(s) for Entity ${entity.metadata.name}. Total number of files: ${allFilesToUpload.length}`);
      } else {
        throw new Error(failed.map((r) => {
          var _a;
          return (_a = r.error) == null ? void 0 : _a.message;
        }).filter(Boolean).join(" "));
      }
    } catch (e) {
      const errorMessage = `Unable to upload file(s) to Azure Blob Storage. ${e}`;
      this.logger.error(errorMessage);
      throw new Error(errorMessage);
    }
  }
  download(containerName, blobPath) {
    return new Promise((resolve, reject) => {
      const fileStreamChunks = [];
      this.storageClient.getContainerClient(containerName).getBlockBlobClient(blobPath).download().then((res) => {
        const body = res.readableStreamBody;
        if (!body) {
          reject(new Error(`Unable to parse the response data`));
          return;
        }
        body.on("error", reject).on("data", (chunk) => {
          fileStreamChunks.push(chunk);
        }).on("end", () => {
          resolve(Buffer.concat(fileStreamChunks));
        });
      }).catch(reject);
    });
  }
  async fetchTechDocsMetadata(entityName) {
    const entityRootDir = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;
    try {
      const techdocsMetadataJson = await this.download(this.containerName, `${entityRootDir}/techdocs_metadata.json`);
      if (!techdocsMetadataJson) {
        throw new Error(`Unable to parse the techdocs metadata file ${entityRootDir}/techdocs_metadata.json.`);
      }
      const techdocsMetadata = JSON5__default['default'].parse(techdocsMetadataJson.toString("utf-8"));
      return techdocsMetadata;
    } catch (e) {
      throw new Error(`TechDocs metadata fetch failed, ${e.message}`);
    }
  }
  docsRouter() {
    return (req, res) => {
      const filePath = decodeURI(req.path.replace(/^\//, ""));
      const fileExtension = path__default['default'].extname(filePath);
      const responseHeaders = getHeadersForFileExtension(fileExtension);
      try {
        this.download(this.containerName, filePath).then((fileContent) => {
          for (const [headerKey, headerValue] of Object.entries(responseHeaders)) {
            res.setHeader(headerKey, headerValue);
          }
          res.send(fileContent);
        });
      } catch (e) {
        this.logger.error(e.message);
        res.status(404).json(e.message);
      }
    };
  }
  hasDocsBeenGenerated(entity) {
    const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
    return this.storageClient.getContainerClient(this.containerName).getBlockBlobClient(`${entityRootDir}/index.html`).exists();
  }
}

class GoogleGCSPublish {
  constructor(storageClient, bucketName, logger) {
    this.storageClient = storageClient;
    this.bucketName = bucketName;
    this.logger = logger;
    this.storageClient = storageClient;
    this.bucketName = bucketName;
    this.logger = logger;
  }
  static fromConfig(config, logger) {
    let bucketName = "";
    try {
      bucketName = config.getString("techdocs.publisher.googleGcs.bucketName");
    } catch (error) {
      throw new Error("Since techdocs.publisher.type is set to 'googleGcs' in your app config, techdocs.publisher.googleGcs.bucketName is required.");
    }
    const credentials = config.getOptionalString("techdocs.publisher.googleGcs.credentials");
    let credentialsJson = {};
    if (credentials) {
      try {
        credentialsJson = JSON.parse(credentials);
      } catch (err) {
        throw new Error("Error in parsing techdocs.publisher.googleGcs.credentials config to JSON.");
      }
    }
    const storageClient = new storage.Storage({
      ...credentials && {
        credentials: credentialsJson
      }
    });
    return new GoogleGCSPublish(storageClient, bucketName, logger);
  }
  async getReadiness() {
    try {
      await this.storageClient.bucket(this.bucketName).getMetadata();
      this.logger.info(`Successfully connected to the GCS bucket ${this.bucketName}.`);
      return {
        isAvailable: true
      };
    } catch (err) {
      this.logger.error(`Could not retrieve metadata about the GCS bucket ${this.bucketName}. Make sure the bucket exists. Also make sure that authentication is setup either by explicitly defining techdocs.publisher.googleGcs.credentials in app config or by using environment variables. Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage`);
      this.logger.error(`from GCS client library: ${err.message}`);
      return {isAvailable: false};
    }
  }
  async publish({entity, directory}) {
    try {
      const allFilesToUpload = await getFileTreeRecursively(directory);
      const limiter = createLimiter__default['default'](10);
      const uploadPromises = [];
      allFilesToUpload.forEach((sourceFilePath) => {
        const relativeFilePath = path__default['default'].relative(directory, sourceFilePath);
        const relativeFilePathPosix = relativeFilePath.split(path__default['default'].sep).join(path__default['default'].posix.sep);
        const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
        const destination = `${entityRootDir}/${relativeFilePathPosix}`;
        const uploadFile = limiter(() => this.storageClient.bucket(this.bucketName).upload(sourceFilePath, {
          destination
        }));
        uploadPromises.push(uploadFile);
      });
      await Promise.all(uploadPromises);
      this.logger.info(`Successfully uploaded all the generated files for Entity ${entity.metadata.name}. Total number of files: ${allFilesToUpload.length}`);
    } catch (e) {
      const errorMessage = `Unable to upload file(s) to Google Cloud Storage. ${e}`;
      this.logger.error(errorMessage);
      throw new Error(errorMessage);
    }
  }
  fetchTechDocsMetadata(entityName) {
    return new Promise((resolve, reject) => {
      const entityRootDir = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;
      const fileStreamChunks = [];
      this.storageClient.bucket(this.bucketName).file(`${entityRootDir}/techdocs_metadata.json`).createReadStream().on("error", (err) => {
        this.logger.error(err.message);
        reject(err);
      }).on("data", (chunk) => {
        fileStreamChunks.push(chunk);
      }).on("end", () => {
        const techdocsMetadataJson = Buffer.concat(fileStreamChunks).toString("utf-8");
        resolve(JSON5__default['default'].parse(techdocsMetadataJson));
      });
    });
  }
  docsRouter() {
    return (req, res) => {
      const filePath = decodeURI(req.path.replace(/^\//, ""));
      const fileExtension = path__default['default'].extname(filePath);
      const responseHeaders = getHeadersForFileExtension(fileExtension);
      this.storageClient.bucket(this.bucketName).file(filePath).createReadStream().on("pipe", () => {
        res.writeHead(200, responseHeaders);
      }).on("error", (err) => {
        this.logger.warn(err.message);
        if (!res.headersSent) {
          res.status(404).send(err.message);
        } else {
          res.destroy();
        }
      }).pipe(res);
    };
  }
  async hasDocsBeenGenerated(entity) {
    return new Promise((resolve) => {
      const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
      this.storageClient.bucket(this.bucketName).file(`${entityRootDir}/index.html`).exists().then((response) => {
        resolve(response[0]);
      }).catch(() => {
        resolve(false);
      });
    });
  }
}

let staticDocsDir = "";
try {
  staticDocsDir = backendCommon.resolvePackagePath("@backstage/plugin-techdocs-backend", "static/docs");
} catch (err) {
  staticDocsDir = os__default['default'].tmpdir();
}
class LocalPublish {
  constructor(config, logger, discovery) {
    this.config = config;
    this.logger = logger;
    this.discovery = discovery;
    this.config = config;
    this.logger = logger;
    this.discovery = discovery;
  }
  async getReadiness() {
    return {
      isAvailable: true
    };
  }
  publish({entity, directory}) {
    var _a;
    const entityNamespace = (_a = entity.metadata.namespace) != null ? _a : "default";
    const publishDir = path__default['default'].join(staticDocsDir, entityNamespace, entity.kind, entity.metadata.name);
    if (!fs__default['default'].existsSync(publishDir)) {
      this.logger.info(`Could not find ${publishDir}, creating the directory.`);
      fs__default['default'].mkdirSync(publishDir, {recursive: true});
    }
    return new Promise((resolve, reject) => {
      fs__default['default'].copy(directory, publishDir, (err) => {
        if (err) {
          this.logger.debug(`Failed to copy docs from ${directory} to ${publishDir}`);
          reject(err);
        }
        this.logger.info(`Published site stored at ${publishDir}`);
        this.discovery.getBaseUrl("techdocs").then((techdocsApiUrl) => {
          resolve({
            remoteUrl: `${techdocsApiUrl}/static/docs/${entity.metadata.name}`
          });
        }).catch((reason) => {
          reject(reason);
        });
      });
    });
  }
  async fetchTechDocsMetadata(entityName) {
    const metadataPath = path__default['default'].join(staticDocsDir, entityName.namespace, entityName.kind, entityName.name, "techdocs_metadata.json");
    try {
      return await fs__default['default'].readJson(metadataPath);
    } catch (err) {
      this.logger.error(`Unable to read techdocs_metadata.json at ${metadataPath}. Error: ${err}`);
      throw new Error(err.message);
    }
  }
  docsRouter() {
    return express__default['default'].static(staticDocsDir, {
      setHeaders: (res, filePath) => {
        const fileExtension = path__default['default'].extname(filePath);
        const headers = getHeadersForFileExtension(fileExtension);
        for (const [header, value] of Object.entries(headers)) {
          res.setHeader(header, value);
        }
      }
    });
  }
  async hasDocsBeenGenerated(entity) {
    var _a;
    const namespace = (_a = entity.metadata.namespace) != null ? _a : "default";
    const indexHtmlPath = path__default['default'].join(staticDocsDir, namespace, entity.kind, entity.metadata.name, "index.html");
    try {
      await fs__default['default'].access(indexHtmlPath, fs__default['default'].constants.F_OK);
      return true;
    } catch (err) {
      return false;
    }
  }
}

const streamToBuffer$1 = (stream) => {
  return new Promise((resolve, reject) => {
    try {
      const chunks = [];
      stream.on("data", (chunk) => chunks.push(chunk));
      stream.on("error", reject);
      stream.on("end", () => resolve(Buffer.concat(chunks)));
    } catch (e) {
      throw new Error(`Unable to parse the response data ${e.message}`);
    }
  });
};
class OpenStackSwiftPublish {
  constructor(storageClient, containerName, logger) {
    this.storageClient = storageClient;
    this.containerName = containerName;
    this.logger = logger;
    this.storageClient = storageClient;
    this.containerName = containerName;
    this.logger = logger;
  }
  static fromConfig(config, logger) {
    let containerName = "";
    try {
      containerName = config.getString("techdocs.publisher.openStackSwift.containerName");
    } catch (error) {
      throw new Error("Since techdocs.publisher.type is set to 'openStackSwift' in your app config, techdocs.publisher.openStackSwift.containerName is required.");
    }
    const openStackSwiftConfig = config.getConfig("techdocs.publisher.openStackSwift");
    const storageClient = pkgcloud.storage.createClient({
      provider: "openstack",
      username: openStackSwiftConfig.getString("credentials.username"),
      password: openStackSwiftConfig.getString("credentials.password"),
      authUrl: openStackSwiftConfig.getString("authUrl"),
      keystoneAuthVersion: openStackSwiftConfig.getOptionalString("keystoneAuthVersion") || "v3",
      domainId: openStackSwiftConfig.getOptionalString("domainId") || "default",
      domainName: openStackSwiftConfig.getOptionalString("domainName") || "Default",
      region: openStackSwiftConfig.getString("region")
    });
    return new OpenStackSwiftPublish(storageClient, containerName, logger);
  }
  getReadiness() {
    return new Promise((resolve) => {
      this.storageClient.getContainer(this.containerName, (err, container) => {
        if (container) {
          this.logger.info(`Successfully connected to the OpenStack Swift container ${this.containerName}.`);
          resolve({
            isAvailable: true
          });
        } else {
          this.logger.error(`Could not retrieve metadata about the OpenStack Swift container ${this.containerName}. Make sure the container exists. Also make sure that authentication is setup either by explicitly defining credentials and region in techdocs.publisher.openStackSwift in app config or by using environment variables. Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage`);
          this.logger.error(`from OpenStack client library: ${err.message}`);
          resolve({
            isAvailable: false
          });
        }
      });
    });
  }
  async publish({entity, directory}) {
    try {
      const allFilesToUpload = await getFileTreeRecursively(directory);
      const limiter = createLimiter__default['default'](10);
      const uploadPromises = [];
      for (const filePath of allFilesToUpload) {
        const relativeFilePath = path__default['default'].relative(directory, filePath);
        const relativeFilePathPosix = relativeFilePath.split(path__default['default'].sep).join(path__default['default'].posix.sep);
        const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
        const destination = `${entityRootDir}/${relativeFilePathPosix}`;
        const params = {
          container: this.containerName,
          remote: destination
        };
        const uploadFile = limiter(() => new Promise((res, rej) => {
          const readStream = fs__default['default'].createReadStream(filePath, "utf8");
          const writeStream = this.storageClient.upload(params);
          writeStream.on("error", rej);
          writeStream.on("success", res);
          readStream.pipe(writeStream);
        }));
        uploadPromises.push(uploadFile);
      }
      await Promise.all(uploadPromises);
      this.logger.info(`Successfully uploaded all the generated files for Entity ${entity.metadata.name}. Total number of files: ${allFilesToUpload.length}`);
      return;
    } catch (e) {
      const errorMessage = `Unable to upload file(s) to OpenStack Swift. ${e}`;
      this.logger.error(errorMessage);
      throw new Error(errorMessage);
    }
  }
  async fetchTechDocsMetadata(entityName) {
    try {
      return await new Promise(async (resolve, reject) => {
        const entityRootDir = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;
        const stream = this.storageClient.download({
          container: this.containerName,
          remote: `${entityRootDir}/techdocs_metadata.json`
        });
        try {
          const techdocsMetadataJson = await streamToBuffer$1(stream);
          if (!techdocsMetadataJson) {
            throw new Error(`Unable to parse the techdocs metadata file ${entityRootDir}/techdocs_metadata.json.`);
          }
          const techdocsMetadata = JSON5__default['default'].parse(techdocsMetadataJson.toString("utf-8"));
          resolve(techdocsMetadata);
        } catch (err) {
          this.logger.error(err.message);
          reject(new Error(err.message));
        }
      });
    } catch (e) {
      throw new Error(`TechDocs metadata fetch failed, ${e.message}`);
    }
  }
  docsRouter() {
    return async (req, res) => {
      const filePath = decodeURI(req.path.replace(/^\//, ""));
      const fileExtension = path__default['default'].extname(filePath);
      const responseHeaders = getHeadersForFileExtension(fileExtension);
      const stream = this.storageClient.download({
        container: this.containerName,
        remote: filePath
      });
      try {
        for (const [headerKey, headerValue] of Object.entries(responseHeaders)) {
          res.setHeader(headerKey, headerValue);
        }
        res.send(await streamToBuffer$1(stream));
      } catch (err) {
        this.logger.warn(err.message);
        res.status(404).send(err.message);
      }
    };
  }
  async hasDocsBeenGenerated(entity) {
    try {
      const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;
      return new Promise((res) => {
        this.storageClient.getFile(this.containerName, `${entityRootDir}/index.html`, (err, file) => {
          if (!err && file) {
            res(true);
          } else {
            res(false);
            this.logger.warn(err.message);
          }
        });
      });
    } catch (e) {
      return Promise.resolve(false);
    }
  }
}

class Publisher {
  static async fromConfig(config, {logger, discovery}) {
    var _a;
    const publisherType = (_a = config.getOptionalString("techdocs.publisher.type")) != null ? _a : "local";
    switch (publisherType) {
      case "googleGcs":
        logger.info("Creating Google Storage Bucket publisher for TechDocs");
        return GoogleGCSPublish.fromConfig(config, logger);
      case "awsS3":
        logger.info("Creating AWS S3 Bucket publisher for TechDocs");
        return AwsS3Publish.fromConfig(config, logger);
      case "azureBlobStorage":
        logger.info("Creating Azure Blob Storage Container publisher for TechDocs");
        return AzureBlobStoragePublish.fromConfig(config, logger);
      case "openStackSwift":
        logger.info("Creating OpenStack Swift Container publisher for TechDocs");
        return OpenStackSwiftPublish.fromConfig(config, logger);
      case "local":
        logger.info("Creating Local publisher for TechDocs");
        return new LocalPublish(config, logger, discovery);
      default:
        logger.info("Creating Local publisher for TechDocs");
        return new LocalPublish(config, logger, discovery);
    }
  }
}

exports.CommonGitPreparer = CommonGitPreparer;
exports.DirectoryPreparer = DirectoryPreparer;
exports.Generators = Generators;
exports.Preparers = Preparers;
exports.Publisher = Publisher;
exports.TechdocsGenerator = TechdocsGenerator;
exports.UrlPreparer = UrlPreparer;
exports.checkoutGitRepository = checkoutGitRepository;
exports.getAzureIntegrationConfig = getAzureIntegrationConfig;
exports.getDefaultBranch = getDefaultBranch;
exports.getDocFilesFromRepository = getDocFilesFromRepository;
exports.getGitHost = getGitHost;
exports.getGitHubIntegrationConfig = getGitHubIntegrationConfig;
exports.getGitLabIntegrationConfig = getGitLabIntegrationConfig;
exports.getGitRepoType = getGitRepoType;
exports.getGitRepositoryTempFolder = getGitRepositoryTempFolder;
exports.getLastCommitTimestamp = getLastCommitTimestamp;
exports.getLocationForEntity = getLocationForEntity;
exports.getTokenForGitRepo = getTokenForGitRepo;
exports.parseReferenceAnnotation = parseReferenceAnnotation;
//# sourceMappingURL=index.cjs.js.map
